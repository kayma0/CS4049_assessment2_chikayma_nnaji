{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2f3af0f9",
   "metadata": {},
   "source": [
    "# Introduction to Machine Learning and Data Mining (CS4049)                                                      \n",
    "## Assessment 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "090dba91",
   "metadata": {},
   "outputs": [],
   "source": [
    "#IMPORTS \n",
    "\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d7c7f15",
   "metadata": {},
   "source": [
    "## 1. Random Sampling.\n",
    "Instructions: Use a random sample of 10K instances drawn from the ~61K instances in the secondary mushroom data, and explain the random sampling code cell(s)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a535c0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #imports pandas so we can read and work with the dataset\n",
    "# import pandas as pd \n",
    "\n",
    "#reads the mushroom data file, and also tells pandas that the data is seperated by semilcolons\n",
    "df = pd.read_csv(\"MushroomDataset/secondary_data.csv\", sep=\";\")\n",
    "\n",
    "# df.head()\n",
    "# df.shape\n",
    "\n",
    "#pick 10k random rows from the whole dataset\n",
    "df_sample = df.sample(n=10000, random_state= 42)\n",
    "\n",
    "#shows how many rows and columns the sample has \n",
    "df_sample.shape\n",
    "\n",
    "# df_sample.head(10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90d84377",
   "metadata": {},
   "source": [
    "The original secondary mushroom dataset (secondary data file) has around 61k rows of data. Working with the full dataset can take more time and use way more computer power when analysing or training machine learning models, or even trying out different experiments. To make the process faster and easier to manage, based on the instructions, I created a smaller version of the data by randomly selecting 10,000 rows out of the original ~61k rows, using the sample() function in pandas.\n",
    "\n",
    "Random sampling is useful because it still keeps the important format and pattern of the full dataset. Since evry row has the same chance of being chosen to represent the full dataset, the 10k sample is shows that it represents the full data well. This means my results should still be meaningful even though I am working with less rows.\n",
    "\n",
    "I also included 'random_state=42' so that the same 10k rows are chosen every time I run the notebook. Doing it this way supports reproducibility, which is an important terminology in machine learning. Reproducibility means getting consistent results when using the same data and methods (National Academies of Sciences, Engineering, and Medicine, 2019). \n",
    "\n",
    "Setting a random state makes sure that anyone who runs my notebook will get the same ample and be able to repeat my results.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2590a029",
   "metadata": {},
   "source": [
    "## 2. Exploratory Data Analysis (EDA).\n",
    "Instructions: Create appropriate visualisations to explore your dataset and summarrise your findings about the data. Highlight findings relevant to the model fitting stage."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "067a95b1",
   "metadata": {},
   "source": [
    "Before traning any machine learning model, it is very important to study the data to understand what the dataset looks like and whether there are any patterns or even potential issues. The mushrrom dataset contains different physical features of mushrooms, and our goal is to predict whether a mushroom is edible or poisonous (the class). These simple checks and visualizations I'll cary out in this question will help me see how the data behaves and what matters when fitting a machine learning model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e31e118e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#checks dastaset structure\n",
    "df_sample.info()\n",
    "\n",
    "#checks for missing values\n",
    "df_sample.isna().sum()\n",
    "\n",
    "# df_sample.head()\n",
    "# df_sample.shape\n",
    "\n",
    "# df_sample.isna().sum()\n",
    "\n",
    "# df_sample['class'].value_counts()\n",
    "\n",
    "# import seaborn as sns\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# sns.countplot (x=df_sample['class'])\n",
    "# plt.title(\"Class Distribution\")\n",
    "# plt.show()\n",
    "\n",
    "# plt.figure(figsize = (8,4))\n",
    "# sns.countplot(x=df_sample['cap-color'])\n",
    "# plt.title(\"Cap Color Distribution\")\n",
    "# plt.show()\n",
    "\n",
    "# plt.figure(figsize = (8,4))\n",
    "# sns.countplot(x=df_sample['habitat'])\n",
    "# plt.title(\"Habitat Distribution\")\n",
    "# plt.show()\n",
    "\n",
    "# import seaborn as sns\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# cols = df_sample.columns\n",
    "\n",
    "# plt.figure(figsize=(18,30))\n",
    "\n",
    "# for i, col in enumerate(cols, 1):\n",
    "#     plt.subplot(7,3, i)\n",
    "#     sns.countplot(x=df_sample[col], order=df_sample[col].value_counts().index)\n",
    "#     plt.title(col)\n",
    "#     plt.xticks(rotation=45, fontsize=8)\n",
    "\n",
    "# plt.tight_layout()\n",
    "# plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f51a2ed",
   "metadata": {},
   "source": [
    "Thoose 2 checks tell me the data types of each field in my dataset and whether there are any missing values. The dataset has no missing values, which makes preprocessing easier before training a machine learning model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "174025bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #this is for importing libraries for the plotting\n",
    "# import seaborn as sns\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "#this creates a bar chart showing how many mushrooms are edible vs those that are poisonous\n",
    "sns.countplot(data = df_sample, x='class')\n",
    "\n",
    "#adds a title to the chart\n",
    "plt.title(\"Edible vs Poisonous\")\n",
    "\n",
    "#displays what its plotted\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16b99263",
   "metadata": {},
   "source": [
    "This plot shows how many mushrooms are edible, and the ones that are poisonous in my sample dataset. The classes look a bit balanced, which is very helpful because having a balanced dataset allows the machine learning model to learn bith classes equally without needing any special techniques for resampling or anything of the sort (Géron, 2019)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "460d393a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#this gets the list of all the non-numeric variables in the dataset\n",
    "category_columns = df_sample.select_dtypes(include='object').columns\n",
    "\n",
    "# for col in category_columns:\n",
    "#     plt.figure(figsize=(6,4))\n",
    "#     sns.countplot(data=df_sample, x=col)\n",
    "#     plt.title(f\"Distribution of {col}\")\n",
    "#     plt.xticks(rotation=45)\n",
    "#     plt.show()\n",
    "\n",
    "#checks how many plots we need\n",
    "num_of_columns = len(category_columns)\n",
    "\n",
    "#this is for setting up the grid, i'm doing for plots per row\n",
    "rows = (num_of_columns//4) +1 \n",
    "plt.figure(figsize=(15, rows*4))\n",
    "\n",
    "#this creates the subplots\n",
    "for i, col in enumerate(category_columns,1):\n",
    "    plt.subplot(rows, 4, i)\n",
    "    sns.countplot(data=df_sample, x=col)\n",
    "    plt.title(f\"Distribution of {col}\")\n",
    "    plt.xticks(rotation=45)\n",
    "\n",
    "#this displays the plots\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3856cc7d",
   "metadata": {},
   "source": [
    "These plots show how each category column (non-numeric column) is distributed across the dataset. Some columns have different categories, while others only have a few common values. Columns that aren't all that different don't neceassarily give the machine learning model alot on information. Columns that show clear differences are more likely to help the model tell the edible and poisionous mushrooms apart."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0ce942a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#shows how gill color appears in each class (edible vs poisonous)\n",
    "sns.countplot(data=df_sample, x='gill-color', hue='class')\n",
    "\n",
    "#Adds a  title\n",
    "plt.title(\"Gill Colour vs Class\")\n",
    "\n",
    "#Turns labels so they'd be easy to read\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "#displays the plot\n",
    "plt.show()\n",
    "\n",
    "sns.countplot(data=df_sample, x='spore-print-color', hue='class')\n",
    "plt.title(\"Spore-Print Colour vs Class\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()\n",
    "\n",
    "# sns.countplot(data=df_sample, x='habitat', hue='class')\n",
    "# plt.title(\"Habitat vs Class\")\n",
    "# plt.xticks(rotation=45)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48798029",
   "metadata": {},
   "source": [
    "The Exploratory Data Analysis (EDA) shows that the dataset is categorical, all the columns/feautures are written as text lables, and there are no missing values, which makes the data easy to work with. I will also need to convert these labels into numbers before trainng a model. The number of edible and poisionous mushroooms is quite balanced, so I don't need to adjust the classes. Some of the feautures like gill-color, and spore-print-color show very cler differences between the 2 classes, so they will be helpful to the machine learning model. Other features hardly change and may not be every helpful. These findings I've gathered guide hpow I will prepate the data the other questions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eba6aaed",
   "metadata": {},
   "source": [
    "## 3. Model Shortlisting based on EDA.\n",
    "Instructions: Based on findings from the EDA task in question 2, shortlist three classifiers, and explain choice of classifiers in terms of your findings from the above EDA task."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf3bdb4c",
   "metadata": {},
   "source": [
    "Once again, the EDA showed that different feautures in the datasset seperate the classes in different ways. Some feautures create very clear splits amd show sharp differences, while others show softer differences. Because of this, it's important to choose classifiers that can capture both simple and more flexible formats and parrtens in the data.\n",
    "\n",
    "I selected three classifiers that make predictions in different ways. This helps me see which style works best for the kinds of patterns I saw in the visalizations made."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a05345c",
   "metadata": {},
   "source": [
    "1. DECISION TREE CLASSIFIER.\n",
    "\n",
    "   A Decision Tree works like a series of simple questions that divide the data into smaller and smaller groups until the machine learning model reaches a final decision. It chooses the questions the questions based on which features split the classes the most. This makes decision trees a very good choice for my dataset beause, in the EDA, some feautures showed clear differences between the edible and poisonos mushrooms. A tree can use these shown strong differences to make correct predictions. This idea is also supported by Qunilan (1986), who describes decision tress as step-by-step 'attribute-based tests' that lead to a final class label.\n",
    "\n",
    "2. RANDOM FOREST CLASSIFIER.\n",
    "   \n",
    "   A Random Forest is a classifier that builds many decision trees and then combines their predictions, making it a forest. Each tree is trained on slightly different data, so the trees learn different patterns. Breiman (2001) explains that a forest works well when the single decision trees are reasonably strong and not way too similar, because the machine learning model can ise the strengths of each decision tree. This reduces errors and improves accuracy. It fits my dataset because the EDA ahowed that several feautures provide useful information, but they don't do that all in the same way. A random Forest can pick up these different patterns across the different trees and produce a more stable and correct result than a single decision tree.\n",
    "\n",
    "3. LOGISTIC REGRESSION CLASSIFIER.\n",
    "   \n",
    "   This is a simple classifier introduced by David Cox (Cox,1958). It is used for yes/no problems, which fits this task because the dataset has two classes (edible or poisonous). The model works by looking at each feauture and estimating how mch it increases or decreases the chance of a mushroom being poisonous. This classifier is a good choice for my dataset because the EDA showed that classes are balanced and that some feautures have clear differences between the two groups. This means a simple classifier like Logistic Regression can still notice these patterns.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a5ea2d8",
   "metadata": {},
   "source": [
    "## 4. Model Fitting.\n",
    "Instructions: Fit the chosen three classifiers to the sample data, and explain your choices and assumptions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cf45318",
   "metadata": {},
   "source": [
    "I trained my three chosen classifiers on the same training data so I could compare them fairly. Before trainingm I changed all the text feautures into numbers, because the models can only work with numeric inputs. I also kept the same 70 - 30 train-test split used in the practicals for each model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7b338c1",
   "metadata": {},
   "source": [
    "1. DECISION TREE.\n",
    "\n",
    "   I fitted the Decision tree directly to the training data. This model learns by making simple \"yes/no\" splits based on the feautures. My assumption is that the clear differences I saw in the EDA will help the tree learn accurate rules. Decision trees do not need scaling (scaling = making all the numbers in the dataset roughly the same size so no feauture with big numbers dominates the others), so I could train it straight away."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e55b6222",
   "metadata": {},
   "outputs": [],
   "source": [
    "#this imports decision tree classifier\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "#this changes all the text feautures into numbers\n",
    "x = pd.get_dummies(df_sample.drop('class', axis=1))\n",
    "\n",
    "# x = df_sample.drop('class', axis=1)...this gave me a Value error\n",
    "\n",
    "#this is the class we want to predict\n",
    "y = df_sample['class']\n",
    "\n",
    "#this line splits the data into training data and test parts\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state=42)\n",
    "\n",
    "#for making a decision tree model\n",
    "dt_model= DecisionTreeClassifier(random_state=42)\n",
    "\n",
    "#for training the decision tree model\n",
    "dt_model.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "341b782b",
   "metadata": {},
   "source": [
    "2. RANDOM FOREST.\n",
    "\n",
    "   Then, I fitted the Random Forest, which builds many decision trees and combines their answers. This helps the model make more stable and accurate predictions. My assumption here is that using many decision trees is better than relying one tree, especially because the dataset has several feautures that go across the two classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa92bb95",
   "metadata": {},
   "outputs": [],
   "source": [
    "#this imports the Random forest classifier\n",
    "#from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "#for making the random forest model\n",
    "rf_model = RandomForestClassifier(n_estimators= 100, random_state= 42)\n",
    "\n",
    "#for training the random forest \n",
    "rf_model.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fc03f02",
   "metadata": {},
   "source": [
    "3. LOGISTIC REGRESSION.\n",
    "\n",
    "   For Logistic Regression, I used a pipeline that scales the data first. then trains the model. Scaling is important for this classifier because it works with weights that depend on the feauture size. My assumption is that some of the class separation can be captured with a simple linear line, which fits what I saw in the EDA. This model also gives me a good baseline to compare with the tree-based models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ae6a7f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#this imports logistic regression and standardscaler\n",
    "# from sklearn.linear_model import LogisticRegression\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "# from sklearn.pipeline import make_pipeline\n",
    "\n",
    "#this is used to prepare the data first, and then run Logistic Regression\n",
    "lr_model = make_pipeline(StandardScaler(), LogisticRegression(max_iter=500, random_state=42))\n",
    "\n",
    "#this is for training the Logistic Regression model\n",
    "lr_model.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3520704",
   "metadata": {},
   "source": [
    "## 5. Model Evaluation and Model Selection.\n",
    "Instructions: Evaluate the three classifiers from the above task using the cross-validation method and explain the performance of the three classifiers. Explain hpw you use the cross-validation results to selct the 'winning' classifier among the three."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2be8988c",
   "metadata": {},
   "source": [
    "1. DECISION TREE EVALUATION.\n",
    "\n",
    "   I used 5 fold cross validation to check how well the Decision Tree works on different parts of the data. A decision tree sometimes learns the training data too closely, so this method helps me see if it performs well again on new data. If the scores change a lot between the 5 folds, it means the tree is not very steady. But if the scores stay high and closely similar, it means the model is good at generalizing (generalizing = the model works well on new data it hasn't seen before)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fd111a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import cross_val_score\n",
    "dt_scores = cross_val_score(dt_model, x, y, cv=5)\n",
    "\n",
    "print(\"Decision Tree Cross Validation Scores: \", dt_scores)\n",
    "\n",
    "print(\"Decision Tree Average Score: \", dt_scores.mean())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e03f90f",
   "metadata": {},
   "source": [
    "2. RANDOM FOREST EVALUATION.\n",
    "\n",
    "   A Random Forest is more stable than a single tree because it uses many decision trees together. Cross-validation with this helps me see if it stays accurate across the different folds of the data. If the scores are high and close to each other, it means the Random Forest is learning the patterns well and is less likely to overfit (overfitting = when a machine learning model learns too much detail and gets confused by new data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc7cd2c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import cross_val_score\n",
    "rf_scores = cross_val_score(rf_model, x, y, cv=5)\n",
    "\n",
    "print(\"Random Forest Cross Validation Scores: \", rf_scores)\n",
    "\n",
    "print(\"Random Forest Average Score: \", rf_scores.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57a0813c",
   "metadata": {},
   "source": [
    "3. LOGISTIC REGRESSION EVALUATION.\n",
    "\n",
    "   Logistic Regression is a simple linear model, so cross-validation in this helps me check if a straight line model is enough for this dataset. If the cross-validation scores are high, it means the classes are easy to seperate with simple patters. But if the scores are lower, then the dataset needs more flexible classifiers, and Logistic Regression won't be the best.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcf6f84e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import cross_val_score\n",
    "lr_scores = cross_val_score(lr_model, x, y, cv=5)\n",
    "\n",
    "print(\"Logistic Regression Cross Validation Scores: \", lr_scores)\n",
    "\n",
    "print(\"Logistic Regression Average Score: \", lr_scores.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d8ede86",
   "metadata": {},
   "source": [
    "MODEL SELECTION.\n",
    "\n",
    "To pick the best model, I looked at the cross-validation scores for each classifier and how similar the scores were across the five folds. A good model should have a high average score and scores that do not change too much.\n",
    "\n",
    "From the results, the Random Forest is clearly the best classifier. It scored [1,1,1,1,1], which means it got every prediction right in all the five folds. The average score is also 1.0, showing perfect performance. This tells me the model is very stable and works well on every part of the data. The Decision Tree and Logistic Regression did not reach this level of correctness or accuracy. \n",
    "\n",
    "Because the Random Forest has the highest score and the most consistent results, it is the best classifier for this dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f6f5d7c",
   "metadata": {},
   "source": [
    "## REFERENCES.\n",
    "1. National Academies of Sciences, Engineering, and Medicine. (2019). Reproducibility and Replicability in Science. Washington, DC: The National Academies Press. p.55.\n",
    "\n",
    "2. Aurélien Géron. (2019). Hands-On Machine Learning with Scikit-Learn, Keras & TensorFlow Concepts, Tools,. 2nd ed. Canada: O’Reilly Media. pp.52-60.\n",
    "\n",
    "3. Peter Waiganjo Wagacha. (2003). Induction of Decision Trees. Nairobi: Institute of Computer Science University of Nairobi. p.3.\n",
    "\n",
    "4. Quinlan, J.R. (1986) 'Induction of decision trees,' Machine Learning, 1(1), pp. 85–87. https://doi.org/10.1023/a:1022643204877.\n",
    "\n",
    "5. Breiman, L. (2001) 'Random forests,' Machine Learning, 45(1), pp. 5–12. https://doi.org/10.1023/a:1010933404324.\n",
    "\n",
    "6. Cox, D.R. (1958) 'The regression analysis of binary sequences,' Journal of the Royal Statistical Society Series B (Statistical Methodology), 20(2), pp. 215–232. https://doi.org/10.1111/j.2517-6161.1958.tb00292.x.\n",
    "\n",
    "7. Logistic Regression · UC Business Analytics R Programming Guide. https://uc-r.github.io/logistic_regression.\n",
    "\n",
    "8. GeeksforGeeks (2017). Cross Validation in Machine Learning. [online] GeeksforGeeks. Available at: https://www.geeksforgeeks.org/machine-learning/cross-validation-machine-learning/.\n",
    "\n",
    "9. GeeksforGeeks (2024). Generalization Rules in AI. [online] GeeksforGeeks. Available at: https://www.geeksforgeeks.org/artificial-intelligence/generalization-rules-in-ai/.\n",
    "\n",
    "10. GeeksforGeeks (2017). ML | Underfitting and Overfitting. [online] GeeksforGeeks. Available at: https://www.geeksforgeeks.org/machine-learning/underfitting-and-overfitting-in-machine-learning/.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
